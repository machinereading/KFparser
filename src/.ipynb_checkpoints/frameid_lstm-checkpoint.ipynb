{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f60e1cd4ab0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from optparse import OptionParser\n",
    "import torch.autograd as autograd\n",
    "import os\n",
    "import sys\n",
    "import os\n",
    "import pprint\n",
    "import numpy as np\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0\"\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True\n",
    "sys.path.insert(0,'../')\n",
    "import preprocessor\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "start_time = time.time()\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "optpr = OptionParser()\n",
    "optpr.add_option(\"--mode\", dest='mode', type='choice', choices=['train', 'test', 'parsing'], default='test')\n",
    "\n",
    "optpr.mode = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: GeForce GTX 1080 , # of gpu:(1)\n",
      "Torch Version: 9.0.176\n"
     ]
    }
   ],
   "source": [
    "print('GPU:', torch.cuda.get_device_name(0), ', # of gpu:('+str(torch.cuda.device_count())+')')\n",
    "print('Torch Version:', torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### loading data now...\n"
     ]
    }
   ],
   "source": [
    "training_data, test_data, dev_data, exemplar_data = preprocessor.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training_data\n",
      " - number of sentences: 3220\n",
      " - number of annotations: 12431 \n",
      "\n",
      "# test_data\n",
      " - number of sentences: 1124\n",
      " - number of annotations: 4382 \n",
      "\n",
      "# dev_data\n",
      " - number of sentences: 183\n",
      " - number of annotations: 624 \n",
      "\n",
      "# exemplar data (from sejong)\n",
      " - number of sentences: 10967\n",
      " - number of annotations: 10967 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "preprocessor.data_stat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### CONFIGURATION ###\n",
      "\n",
      "{'batch_size': 64,\n",
      " 'hidden_dim': 64,\n",
      " 'learning_rate': 0.001,\n",
      " 'lstm_depth': 2,\n",
      " 'lstm_dim': 64,\n",
      " 'lstm_input_dim': 64,\n",
      " 'lu_dim': 64,\n",
      " 'lu_pos_dim': 5,\n",
      " 'num_epochs': 100,\n",
      " 'pos_dim': 4,\n",
      " 'token_dim': 60,\n",
      " 'using_GPU': True}\n"
     ]
    }
   ],
   "source": [
    "configuration = {'token_dim': 60,\n",
    "                 'hidden_dim': 64,\n",
    "                 'pos_dim': 4,\n",
    "                 'lu_dim': 64,\n",
    "                 'lu_pos_dim': 5,\n",
    "                 'lstm_input_dim': 64,\n",
    "                 'lstm_dim': 64,\n",
    "                 'lstm_depth': 2,\n",
    "                 'hidden_dim': 64,\n",
    "                 'num_epochs': 100,\n",
    "                 'learning_rate': 0.001,\n",
    "                 'using_GPU': True,\n",
    "                 'batch_size': 64}\n",
    "print('\\n### CONFIGURATION ###\\n')\n",
    "pprint.pprint(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.date.today()\n",
    "model_dir = './model/lstm-'+str(today)\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "model_path = model_dir+'/model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper-parameters\n",
    "usingGPU = configuration['using_GPU']\n",
    "TOKDIM= configuration['token_dim']\n",
    "POSDIM = configuration['pos_dim']\n",
    "LUDIM = configuration['lu_dim']\n",
    "LPDIM = configuration['lu_pos_dim']\n",
    "INPDIM = LUDIM + LPDIM\n",
    "LSTMINPDIM = configuration['lstm_input_dim']\n",
    "LSTMDIM = configuration['lstm_dim']\n",
    "LSTMDEPTH = configuration['lstm_depth']\n",
    "HIDDENDIM = configuration['hidden_dim']\n",
    "NUM_EPOCHS = configuration['num_epochs']\n",
    "learning_rate = configuration['learning_rate']\n",
    "batch_size = configuration['batch_size']\n",
    "\n",
    "# num_layers = 1\n",
    "# num_epochs = 5\n",
    "# num_samples = 1000     # number of words to be sampled\n",
    "# batch_size = 20\n",
    "# seq_length = 30\n",
    "seq_length = 0\n",
    "for i in training_data:\n",
    "    new_len = len(i)\n",
    "    if new_len > seq_length:\n",
    "        seq_length = new_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unseen word, <PAD> 에 대한 index 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###  word vocab size: 21172\n",
      "### frame vocab size: 657\n"
     ]
    }
   ],
   "source": [
    "def prepare_index():\n",
    "    word_to_ix = {}\n",
    "    pos_to_ix = {}\n",
    "    frame_to_ix = {}\n",
    "    word_to_ix['UNSEEN'] = 0\n",
    "    word_vocab, pos_vocab, frame_vocab = [],[],[]\n",
    "    for tokens in training_data:\n",
    "        for t in tokens:\n",
    "            word = t[1]\n",
    "            if word not in word_to_ix:\n",
    "                word_to_ix[word] = len(word_to_ix)\n",
    "                word_vocab.append(word)\n",
    "            pos = t[4]\n",
    "            if pos not in pos_to_ix:\n",
    "                pos_to_ix[pos] = len(pos_to_ix)\n",
    "                pos_vocab.append(pos)\n",
    "            frame = t[13]\n",
    "            if frame != '_':\n",
    "                if frame not in frame_to_ix:\n",
    "                    frame_to_ix[frame] = len(frame_to_ix)\n",
    "                    frame_vocab.append(frame)\n",
    "    return word_to_ix, pos_to_ix, frame_to_ix\n",
    "word_to_ix, pos_to_ix, frame_to_ix = prepare_index()\n",
    "print('\\n###  word vocab size:', len(word_to_ix))\n",
    "print('### frame vocab size:', len(frame_to_ix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sentence(tokens):\n",
    "    sentence, pos, frame = [],[],[]\n",
    "    for token in tokens:\n",
    "        w,p,f = token[1],token[4],token[13]\n",
    "        sentence.append(w)\n",
    "        pos.append(p)\n",
    "        frame.append(f)\n",
    "    return sentence, pos, frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    vocab = list(to_ix.keys())\n",
    "    idxs = []\n",
    "    for w in seq:\n",
    "        if w in vocab:\n",
    "            idxs.append(to_ix[w])\n",
    "        else:\n",
    "            idxs.append(0)            \n",
    "#     idxs = [to_ix[w] for w in seq]\n",
    "    if usingGPU:\n",
    "        return torch.tensor(idxs).type(torch.cuda.LongTensor)\n",
    "    else:\n",
    "        return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_frame_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq if w != '_']\n",
    "    idxs = list(set(idxs))\n",
    "    if usingGPU:\n",
    "        return torch.tensor(idxs).type(torch.cuda.LongTensor)\n",
    "    else:\n",
    "        return torch.tensor(idxs, dtype=torch.long)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def gen_sequence_data(data, bs=batch_size):\n",
    "# #     print('\\n### generate batch data ###')\n",
    "# #     print('batch size:', bs)\n",
    "# #     num_batches = len(data) // bs\n",
    "# #     print('num batches:', num_batches)\n",
    "\n",
    "\n",
    "# def gen_sequence_data(data, bs=batch_size):\n",
    "#     print('\\n### generate batch data ###')\n",
    "#     print('batch size:', bs)\n",
    "#     num_batches = len(data) // bs\n",
    "#     print('num batches:', num_batches)\n",
    "#     seq_data = []\n",
    "#     for tokens in data:\n",
    "#         sentence, pos, frame = prepare_sentence(tokens)\n",
    "#         sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "#         frames = prepare_frame_sequence(frame, frame_to_ix)\n",
    "#         targetpositions = get_targetpositions(tokens)\n",
    "#         seq_data.append( (sentence_in, frames, targetpositions) )\n",
    "# #     return seq_data\n",
    "#     seq_data = seq_data[:num_batches*bs]\n",
    "    \n",
    "#     n = 0\n",
    "#     batch_data = []\n",
    "#     sent_list, frame_list, targetpositions = [],[],[]\n",
    "#     for (sent,frame,targetposition) in rev_data:\n",
    "#         sent_list.append(sent)\n",
    "#         frame_list.append(frame)\n",
    "#         targetpositions.append(targetposition)\n",
    "#         n += 1\n",
    "#         if n % bs == 0:\n",
    "#             intuple = ( sent_list, frame_list, targetposition )\n",
    "#             batch_data.append(intuple)\n",
    "#             sent_list, frame_list = [],[]\n",
    "#     print(len(batch_data))\n",
    "#     return batch_data\n",
    "\n",
    "\n",
    "# d = gen_sequence_data(training_data)\n",
    "\n",
    "\n",
    "\n",
    "# ## TODO 0803: batch data 다시만들기\n",
    "# #batch 0 : 1개 튜플, 각 튜플에는 sentence tensor 64개, frame tensor 64개\n",
    "# # 그리고 padding 까지 해서 --> 주말 돌림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_data = gen_sequence_data(training_data)\n",
    "# n = 0\n",
    "# for i in seq_data:\n",
    "#     n += 1\n",
    "#     if n >= 5:\n",
    "#         print(i)\n",
    "#         print('\\n ###')\n",
    "        \n",
    "#     if n > 11:\n",
    "#         break\n",
    "\n",
    "# print(seq_data[0])\n",
    "\n",
    "\n",
    "# batch_data = torch.utils.data.DataLoader(seq_data, batch_size=2)\n",
    "\n",
    "# print('batch')\n",
    "# n = 0\n",
    "# for i in batch_data:\n",
    "#     print(i)\n",
    "#     n += 1\n",
    "#     if n >= 11:\n",
    "#         print(i)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_frame_vector(seq, to_ix):\n",
    "    if usingGPU:\n",
    "        frame_vector =  torch.zeros(len(to_ix)).type(torch.cuda.LongTensor)\n",
    "#         frame_vector[0][fid] = 1\n",
    "    else:\n",
    "        frame_vector =  torch.zeros(len(to_ix), dtype=torch.long)\n",
    "#         frame_vector[0][fid] = 1\n",
    "    for f in seq:\n",
    "        if f != '_':\n",
    "            fid = frame_to_ix[f]\n",
    "            frame_vector[fid] = 1\n",
    "    return frame_vector\n",
    "\n",
    "# _, _, seq = prepare_sentence(training_data[50])\n",
    "# d = prepare_frame_vector(seq, frame_to_ix)\n",
    "# print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targetpositions(tokens):\n",
    "    positions = []\n",
    "    for i in tokens:\n",
    "        if i[12] != '_':\n",
    "            positions.append(int(i[0]))\n",
    "    positions = np.asarray(positions)\n",
    "    positions = torch.from_numpy(positions)\n",
    "    if usingGPU:\n",
    "        return positions.type(torch.cuda.LongTensor)\n",
    "    else:\n",
    "        return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_span(sentence, targetpositions):\n",
    "    start, end = int(targetpositions[0]), int(targetpositions[-1])\n",
    "    span = {}\n",
    "    if start == 0: span['start'] = 0\n",
    "    else: span['start'] = start -1\n",
    "    if end == len(sentence): span['end'] = end+1\n",
    "    else: span['end'] = end+2\n",
    "    return span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, TOKDIM)\n",
    "        \n",
    "        # 1st LSTM network (bi-LSTM)\n",
    "        self.lstm_1 = nn.LSTM(TOKDIM, HIDDENDIM//2, bidirectional=True)\n",
    "        self.hidden_lstm_1 = self.init_hidden_lstm_1()\n",
    "        \n",
    "        # 2nd LSTM network (LSTM)\n",
    "        self.hidden_lstm_2 = self.init_hidden_lstm_2()\n",
    "        self.lstm_2 = nn.LSTM(HIDDENDIM, HIDDENDIM)\n",
    "        \n",
    "        self.target2hidden = nn.Linear(HIDDENDIM, HIDDENDIM)\n",
    "        self.hidden2tag = nn.Linear(HIDDENDIM, tagset_size) \n",
    "    \n",
    "    def init_hidden(self):\n",
    "        if usingGPU:\n",
    "            return (torch.zeros(1, 1, HIDDENDIM).cuda(),\n",
    "                torch.zeros(1, 1, HIDDENDIM).cuda())\n",
    "        else:\n",
    "            return (torch.zeros(1, 1, HIDDENDIM),\n",
    "                torch.zeros(1, 1, HIDDENDIM))\n",
    "    \n",
    "    def init_hidden_lstm_1(self):\n",
    "        if usingGPU:\n",
    "            return (torch.zeros(2, 1, HIDDENDIM//2).cuda(),\n",
    "                torch.zeros(2, 1, HIDDENDIM//2).cuda())\n",
    "        else:\n",
    "            return (torch.zeros(2, 1, HIDDENDIM//2),\n",
    "                torch.zeros(2, 1, HIDDENDIM//2))\n",
    "        \n",
    "    def init_hidden_lstm_2(self):\n",
    "        if usingGPU:\n",
    "            return (torch.zeros(1, 1, HIDDENDIM).cuda(),\n",
    "                torch.zeros(1, 1, HIDDENDIM).cuda())\n",
    "        else:\n",
    "            return (torch.zeros(1, 1, HIDDENDIM),\n",
    "                torch.zeros(1, 1, HIDDENDIM))\n",
    "        \n",
    "    def forward(self, sentence, targetpositions):\n",
    "#         if usingGPU: \n",
    "#             sentence.cuda()\n",
    "#         else: \n",
    "#             pass\n",
    "\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        embeds = embeds.view(len(sentence), 1, -1)\n",
    "        \n",
    "        self.lstm_1.flatten_parameters()        \n",
    "        lstm_out_1, self.hidden_lstm_1 = self.lstm_1(\n",
    "            embeds, self.hidden_lstm_1)\n",
    "\n",
    "        span = get_target_span(sentence, targetpositions)  \n",
    "\n",
    "        target_lstm = lstm_out_1[span['start']:span['end']]\n",
    "        \n",
    "\n",
    "        self.lstm_2.flatten_parameters()    \n",
    "        lstm_out_2, self.hidden = self.lstm_2(\n",
    "            target_lstm, self.hidden_lstm_2)        \n",
    "\n",
    "        target_vec = lstm_out_2[-1]\n",
    "\n",
    "        tag_space = self.target2hidden(target_vec)        \n",
    "        tag_space = F.relu(tag_space)\n",
    "        tag_space = self.hidden2tag(tag_space)\n",
    "#         tag_space = F.softmax(tag_space)\n",
    "#         tag_space = autograd.Variable(tag_space, requires_grad=True)\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frame_by_tensor(t):\n",
    "    value, indices = t.max(1)\n",
    "    score = pow(10, value)\n",
    "    \n",
    "    pred = None\n",
    "    for frame, idx in frame_to_ix.items():\n",
    "        if idx == indices:\n",
    "            pred = frame\n",
    "            break\n",
    "    return score, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_dev(model):\n",
    "    acc = 0\n",
    "    for sent in dev_data:\n",
    "        for t in sent:\n",
    "            if t[13] != '_':\n",
    "                gold = t[13]\n",
    "        sentence, pos, frame = prepare_sentence(sent)\n",
    "        targetpositions = get_targetpositions(sent)\n",
    "        inputs = prepare_sequence(sentence, word_to_ix)\n",
    "        tag_scores = model(inputs,targetpositions)\n",
    "        score, pred = get_frame_by_tensor(tag_scores)\n",
    "        if gold == pred:\n",
    "            acc += 1\n",
    "        else:\n",
    "            pass\n",
    "    accuracy = acc / len(dev_data)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Accuracy: 0.000000\n",
      "after\n",
      "tensor([[-6.4437, -6.3149, -6.4660, -6.4522, -6.5263, -6.5322, -6.6079, -6.4930,\n",
      "         -6.5078, -6.4722, -6.5608, -6.4599, -6.6442, -6.5374, -6.3920, -6.4233,\n",
      "         -6.6157, -6.5270, -6.5999, -6.5725, -6.5034, -6.4281, -6.5617, -6.4362,\n",
      "         -6.6169, -6.5231, -6.5714, -6.4637, -6.4134, -6.3757, -6.5577, -6.6076,\n",
      "         -6.6188, -6.5551, -6.5577, -6.4999, -6.5226, -6.3756, -6.6032, -6.6550,\n",
      "         -6.3194, -6.5940, -6.4979, -6.4189, -6.5936, -6.3836, -6.4579, -6.4092,\n",
      "         -6.4903, -6.4782, -6.3965, -6.4526, -6.5914, -6.4306, -6.3912, -6.5702,\n",
      "         -6.4512, -6.5573, -6.5677, -6.4640, -6.5483, -6.5321, -6.6072, -6.4601,\n",
      "         -6.6371, -6.4048, -6.3847, -6.4155, -6.5820, -6.4516, -6.5607, -6.3923,\n",
      "         -6.4819, -6.4582, -6.4290, -6.4270, -6.6126, -6.3815, -6.5077, -6.4919,\n",
      "         -6.6117, -6.6033, -6.4836, -6.4017, -6.4904, -6.3764, -6.4263, -6.4893,\n",
      "         -6.5984, -6.5223, -6.6033, -6.5821, -6.4220, -6.3950, -6.6181, -6.5614,\n",
      "         -6.6015, -6.4479, -6.4547, -6.3765, -6.4301, -6.4986, -6.4490, -6.4411,\n",
      "         -6.4607, -6.5728, -6.5704, -6.5261, -6.3644, -6.5849, -6.3738, -6.5681,\n",
      "         -6.5120, -6.5603, -6.4905, -6.4827, -6.5872, -6.5690, -6.3974, -6.3897,\n",
      "         -6.4496, -6.5553, -6.5385, -6.4452, -6.4633, -6.4616, -6.5109, -6.5264,\n",
      "         -6.5476, -6.5880, -6.4624, -6.5697, -6.5073, -6.4568, -6.4477, -6.5719,\n",
      "         -6.3925, -6.3803, -6.4418, -6.5496, -6.5728, -6.5819, -6.4667, -6.4992,\n",
      "         -6.6038, -6.5110, -6.4470, -6.5648, -6.4120, -6.6268, -6.5803, -6.3878,\n",
      "         -6.5274, -6.3640, -6.4305, -6.5584, -6.6372, -6.4451, -6.3419, -6.5057,\n",
      "         -6.6083, -6.4299, -6.4445, -6.4002, -6.5516, -6.4842, -6.4578, -6.3811,\n",
      "         -6.5414, -6.3974, -6.5622, -6.6114, -6.3653, -6.5413, -6.3816, -6.5341,\n",
      "         -6.4302, -6.3794, -6.5239, -6.5118, -6.5700, -6.4247, -6.3811, -6.3813,\n",
      "         -6.6481, -6.4953, -6.5769, -6.3877, -6.5724, -6.4698, -6.4436, -6.5124,\n",
      "         -6.4414, -6.4241, -6.4972, -6.4174, -6.4796, -6.3881, -6.5808, -6.6487,\n",
      "         -6.5693, -6.6028, -6.4272, -6.3667, -6.4317, -6.4955, -6.5898, -6.5458,\n",
      "         -6.5576, -6.5290, -6.4992, -6.3579, -6.4261, -6.6365, -6.3190, -6.4656,\n",
      "         -6.4979, -6.4517, -6.4641, -6.4298, -6.6212, -6.5383, -6.4658, -6.5135,\n",
      "         -6.4817, -6.4084, -6.5579, -6.5546, -6.5872, -6.5605, -6.4767, -6.5978,\n",
      "         -6.4920, -6.5461, -6.5338, -6.4466, -6.4201, -6.4064, -6.6362, -6.3596,\n",
      "         -6.4832, -6.5487, -6.4649, -6.5327, -6.5111, -6.3267, -6.5542, -6.3664,\n",
      "         -6.5592, -6.5568, -6.5344, -6.4027, -6.4321, -6.4606, -6.4167, -6.6056,\n",
      "         -6.5216, -6.3770, -6.4150, -6.5479, -6.3670, -6.4398, -6.4414, -6.4346,\n",
      "         -6.6114, -6.5302, -6.4765, -6.5229, -6.3898, -6.4061, -6.3749, -6.4286,\n",
      "         -6.6030, -6.4983, -6.4397, -6.4104, -6.6378, -6.5611, -6.5186, -6.5959,\n",
      "         -6.5808, -6.5344, -6.4693, -6.5522, -6.3624, -6.4558, -6.5031, -6.5004,\n",
      "         -6.6213, -6.4876, -6.4117, -6.5375, -6.3767, -6.5320, -6.6491, -6.4474,\n",
      "         -6.5910, -6.5394, -6.5660, -6.5668, -6.5530, -6.4713, -6.4747, -6.4595,\n",
      "         -6.4167, -6.3834, -6.4027, -6.4618, -6.4121, -6.4674, -6.5008, -6.4339,\n",
      "         -6.4812, -6.4061, -6.5316, -6.3712, -6.4426, -6.5956, -6.5323, -6.6223,\n",
      "         -6.5794, -6.4680, -6.4282, -6.4478, -6.6252, -6.5547, -6.4558, -6.4779,\n",
      "         -6.3827, -6.5430, -6.6089, -6.5898, -6.5089, -6.4271, -6.4701, -6.4920,\n",
      "         -6.5448, -6.4549, -6.5177, -6.3912, -6.5323, -6.5239, -6.4515, -6.5143,\n",
      "         -6.4864, -6.3926, -6.3979, -6.4454, -6.5837, -6.2928, -6.5776, -6.4291,\n",
      "         -6.5916, -6.5032, -6.3929, -6.4016, -6.4515, -6.6755, -6.4038, -6.4955,\n",
      "         -6.5291, -6.4363, -6.6227, -6.3776, -6.4203, -6.5709, -6.3682, -6.6184,\n",
      "         -6.4460, -6.5700, -6.4362, -6.3746, -6.5607, -6.3597, -6.3918, -6.4289,\n",
      "         -6.5369, -6.3368, -6.5161, -6.5289, -6.3898, -6.5235, -6.6655, -6.5502,\n",
      "         -6.4447, -6.4749, -6.6196, -6.4993, -6.4292, -6.5144, -6.4701, -6.4763,\n",
      "         -6.4240, -6.5875, -6.5169, -6.6316, -6.3432, -6.4673, -6.4859, -6.6138,\n",
      "         -6.5413, -6.5626, -6.4100, -6.5993, -6.3535, -6.4911, -6.5071, -6.5017,\n",
      "         -6.5832, -6.4247, -6.5011, -6.5709, -6.4645, -6.6027, -6.5685, -6.3453,\n",
      "         -6.4829, -6.4467, -6.4122, -6.5126, -6.6642, -6.5829, -6.4848, -6.3307,\n",
      "         -6.4626, -6.5706, -6.5280, -6.5074, -6.4250, -6.6346, -6.3793, -6.4755,\n",
      "         -6.4994, -6.4082, -6.3869, -6.5265, -6.5213, -6.4746, -6.3554, -6.4056,\n",
      "         -6.3997, -6.3363, -6.6259, -6.5813, -6.5491, -6.5614, -6.5672, -6.5838,\n",
      "         -6.3660, -6.5699, -6.5042, -6.5472, -6.6067, -6.4852, -6.5547, -6.4903,\n",
      "         -6.4550, -6.4911, -6.4278, -6.4321, -6.5835, -6.4966, -6.5746, -6.4539,\n",
      "         -6.5908, -6.5466, -6.4398, -6.4691, -6.5687, -6.5209, -6.4636, -6.5486,\n",
      "         -6.4419, -6.4105, -6.4080, -6.5067, -6.5442, -6.4569, -6.5537, -6.5346,\n",
      "         -6.4395, -6.4720, -6.4562, -6.4904, -6.3913, -6.5196, -6.4144, -6.5108,\n",
      "         -6.5257, -6.6057, -6.4053, -6.5132, -6.4613, -6.4868, -6.5940, -6.5443,\n",
      "         -6.4879, -6.6122, -6.4999, -6.4015, -6.5372, -6.4928, -6.5226, -6.4441,\n",
      "         -6.5815, -6.4851, -6.4762, -6.3939, -6.4800, -6.4215, -6.4029, -6.4012,\n",
      "         -6.3497, -6.6791, -6.5248, -6.6172, -6.6128, -6.4027, -6.4946, -6.4707,\n",
      "         -6.6850, -6.5766, -6.5200, -6.3459, -6.5446, -6.5420, -6.4663, -6.4929,\n",
      "         -6.4048, -6.5364, -6.3221, -6.4899, -6.4699, -6.5819, -6.3030, -6.5151,\n",
      "         -6.5630, -6.5706, -6.4379, -6.3915, -6.3409, -6.6085, -6.4256, -6.4586,\n",
      "         -6.5044, -6.5907, -6.4982, -6.3566, -6.5208, -6.5395, -6.4675, -6.5489,\n",
      "         -6.4380, -6.6103, -6.5655, -6.5758, -6.3917, -6.5393, -6.5637, -6.3601,\n",
      "         -6.5475, -6.3984, -6.5217, -6.4591, -6.3884, -6.5870, -6.5809, -6.6016,\n",
      "         -6.5085, -6.4805, -6.4934, -6.2965, -6.5792, -6.4875, -6.4742, -6.4163,\n",
      "         -6.4013, -6.4854, -6.5101, -6.4445, -6.4125, -6.3468, -6.3806, -6.5596,\n",
      "         -6.4503, -6.4948, -6.4254, -6.4458, -6.5174, -6.6423, -6.4056, -6.6349,\n",
      "         -6.4326, -6.4815, -6.5634, -6.4290, -6.3772, -6.3463, -6.4949, -6.5198,\n",
      "         -6.4023, -6.3509, -6.6348, -6.5516, -6.4554, -6.3783, -6.5885, -6.4526,\n",
      "         -6.3919, -6.6040, -6.5349, -6.4792, -6.5338, -6.4469, -6.3278, -6.3900,\n",
      "         -6.4208, -6.5300, -6.4213, -6.3951, -6.4979, -6.4600, -6.5988, -6.5289,\n",
      "         -6.5787, -6.5328, -6.4757, -6.4412, -6.4689, -6.6422, -6.6638, -6.3022,\n",
      "         -6.4147, -6.3900, -6.4271, -6.5389, -6.6379, -6.5889, -6.4461, -6.6683,\n",
      "         -6.5550, -6.4353, -6.5397, -6.4293, -6.5639, -6.5490, -6.3521, -6.3261,\n",
      "         -6.5174, -6.5114, -6.4289, -6.4095, -6.4810, -6.5520, -6.5724, -6.3741,\n",
      "         -6.3950]], device='cuda:0')\n",
      "\n",
      "\n",
      "your model is saved: ./model/lstm-2018-08-06/model.pt\n",
      "time spent: 47.65811491012573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type LSTMTagger. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# training_data = training_data[:100]\n",
    "# total_step = len(training_data)\n",
    "# NUM_EPOCHS = 3\n",
    "# EMBEDDING_DIM = 6\n",
    "# HIDDEN_DIM = 6\n",
    "\n",
    "# seq_data = gen_sequence_data(training_data)\n",
    "# batch_data = torch.utils.data.DataLoader(seq_data, batch_size=batch_size)\n",
    "# total_step = len(batch_data)\n",
    "# for epoch in range(NUM_EPOCHS):\n",
    "#     for step, (sent, frame, targetposition) in enumerate(batch_data):\n",
    "#         model.zero_grad()\n",
    "#         model.hidden_lstm_1 = model.init_hidden_lstm_1()\n",
    "#         model.hidden_lstm_2 = model.init_hidden_lstm_2()\n",
    "        \n",
    "#         tag_scores = model(sent, targetposition)\n",
    "#         loss = loss_function(tag_scores, frames)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()     \n",
    "\n",
    "#         if n % 10 == 0:\n",
    "#             print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "#                    .format(epoch+1, NUM_EPOCHS, step, total_step, loss.item()))\n",
    "#         break\n",
    "\n",
    "model = False\n",
    "optpr.mode = 'train'\n",
    "if optpr.mode == 'train':\n",
    "    model = LSTMTagger(len(word_to_ix), len(frame_to_ix))\n",
    "    if usingGPU:\n",
    "        model.cuda()\n",
    "    else:\n",
    "        pass\n",
    "    loss_function = nn.NLLLoss()\n",
    "#     loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    total_step = len(training_data)\n",
    "    dev_eval = []\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        n = 0\n",
    "        for tokens in training_data:\n",
    "            model.zero_grad()\n",
    "            model.hidden_lstm_1 = model.init_hidden_lstm_1()\n",
    "            model.hidden_lstm_2 = model.init_hidden_lstm_2()\n",
    "\n",
    "            sentence, pos, frame = prepare_sentence(tokens)\n",
    "            targetpositions = get_targetpositions(tokens)\n",
    "            sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "            frames = prepare_frame_sequence(frame, frame_to_ix)\n",
    "    #         frames = prepare_frame_vector(frame, frame_to_ix)\n",
    "            tag_scores = model(sentence_in, targetpositions)\n",
    "            loss = loss_function(tag_scores, frames)\n",
    "            loss.backward()\n",
    "            optimizer.step()        \n",
    "            n = n+1\n",
    "\n",
    "            if n % 100 == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                       .format(epoch+1, NUM_EPOCHS, n, total_step, loss.item()))\n",
    "            \n",
    "        accuracy = eval_dev(model)\n",
    "        print('Epoch [{}/{}], Accuracy: {:4f}' \n",
    "                       .format(epoch+1, NUM_EPOCHS, accuracy))\n",
    "        dev_eval.append((str(epoch+1), str(accuracy)))\n",
    "    #         break\n",
    "\n",
    "    torch.save(model, model_path)  \n",
    "\n",
    "    print('your model is saved:', model_path)\n",
    "    print('time spent:', time.time()-start_time)\n",
    "    with open(model_dir+'/configure.json','w') as f:\n",
    "        json.dump(configuration, f, ensure_ascii=False, indent=4)\n",
    "    eval_dev_epoch = open(model_dir+'/eval_dev_epoch.tsv', 'w')\n",
    "    for i in dev_eval:\n",
    "        eval_dev_epoch.write(i[0]+'\\t'+i[1]+'\\n')\n",
    "    eval_dev_epoch.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if optpr.mode == 'test' or model != False:\n",
    "    model = torch.load(model_path)\n",
    "    acc = 0\n",
    "    for sent in test_data:\n",
    "        for t in sent:\n",
    "            if t[13] != '_':\n",
    "                gold = t[13]\n",
    "        sentence, pos, frame = prepare_sentence(sent)\n",
    "        targetpositions = get_targetpositions(sent)\n",
    "        inputs = prepare_sequence(sentence, word_to_ix)\n",
    "        tag_scores = model(inputs,targetpositions)\n",
    "        score, pred = get_frame_by_tensor(tag_scores)\n",
    "        if gold == pred:\n",
    "            acc += 1\n",
    "        else:\n",
    "            pass\n",
    "    accuracy = acc / len(test_data)\n",
    "    with open(model_dir+'/result','w') as f:\n",
    "        line = 'accuracy: '+str(accuracy)\n",
    "        f.write(line)\n",
    "    print('accuracy over TEST:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LU candidate 중에 대해서만 softmax 를 하고, 그중에 하나만 고르게 해야됨\n",
    "# 평가에서도 accuracy이므로, lu 중에 모호성이 있는것에 대해서만 해야됨\n",
    "\n",
    "이걸 하는 법: masked softmax\n",
    "https://discuss.pytorch.org/t/apply-mask-softmax/14212/13"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
