{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from src import etri\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import time\n",
    "from koreanframenet import kfn\n",
    "from optparse import OptionParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import targetid\n",
    "import frameid\n",
    "import argid\n",
    "import graphGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PARSER SETTINGS\n",
      "_____________________\n",
      "Target Identification     \tbaseline\n",
      "Frame Identification      \tfrequent\n",
      "Argument Identification   \trulebased\n",
      "RESULT WILL BE SAVED TO\t./tmp/2018.7.27.1532656285.8931777\n"
     ]
    }
   ],
   "source": [
    "# options\n",
    "\n",
    "now = datetime.now()\n",
    "resultfname = './tmp/'+str(now.year)+'.'+str(now.month)+'.'+str(now.day)+'.'+str(time.time())\n",
    "\n",
    "optpr = OptionParser()\n",
    "optpr.add_option(\"--mode\", dest='mode', type='choice', choices=['parsing', 'test'], default='parsing')\n",
    "optpr.add_option(\"--targetid\", dest='targetid', type='choice', choices=['baseline'], default='baseline')\n",
    "optpr.add_option(\"--frameid\", dest='frameid', type='choice', choices=['baseline'], default='baseline')\n",
    "optpr.add_option(\"--argid\", dest='argid', type='choice', choices=['baseline'], default='baseline')\n",
    "optpr.add_option(\"--result\", dest='resultfile', help=\"Saved result file\", metavar=\"FILE\", default=resultfname)\n",
    "optpr.add_option(\"--input\", dest='input', help=\"input a sentence\", type=\"string\", default=resultfname)\n",
    "# (options, args) = optpr.parse_args()\n",
    "\n",
    "optpr.mode = 'parsing'\n",
    "optpr.targetid = 'baseline'\n",
    "optpr.frameid = 'frequent'\n",
    "optpr.argid = 'rulebased'\n",
    "#optpr.argid = 'suffix_only'\n",
    "\n",
    "#print options\n",
    "#sys.stderr.write(\"\\nCOMMAND: \"+' '.join(sys.argv) + '\\n')\n",
    "sys.stderr.write(\"\\nPARSER SETTINGS\\n_____________________\\n\")\n",
    "sys.stderr.write(\"Target Identification     \\t\" + str(optpr.targetid) + '\\n')\n",
    "sys.stderr.write(\"Frame Identification      \\t\" + str(optpr.frameid) + '\\n')\n",
    "sys.stderr.write(\"Argument Identification   \\t\" + str(optpr.argid) + '\\n')\n",
    "if optpr.mode in ['parsing']:\n",
    "    sys.stderr.write(\"RESULT WILL BE SAVED TO\\t%s\\n\" %resultfname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parset options\n",
    "\n",
    "target_identifier = targetid.target_identifier\n",
    "frame_identifier = frameid.frame_identifier\n",
    "arg_identifier = argid.arg_identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frameparsing(sent):\n",
    "    conll = etri.getETRI_CoNLL2009(sent)\n",
    "    conll_target = target_identifier(conll, optpr.targetid)\n",
    "    conll_frame = frame_identifier(conll_target, optpr.frameid)\n",
    "    conll_arg = arg_identifier(conll_frame, optpr.argid)\n",
    "    \n",
    "    return conll_arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('frame:Political_locales', 'frdf:lu', '마을.n')\n",
      "('frame:Being_born', 'frdf:lu', '태어나다.v')\n"
     ]
    }
   ],
   "source": [
    "# parsing\n",
    "\n",
    "def test():\n",
    "    if optpr.mode == 'parsing':\n",
    "        #sent = optpr.input = \"기계 학습(機械學習) 또는 머신 러닝(영어: machine learning)은 인공 지능의 한 분야로, 기계가 정보를 학습하도록 하는 알고리즘과 기술을 개발하는 분야를 말한다.\"\n",
    "        sent = \"지미_카터 는 조지아_주  섬터 카운티 플레인스 마을에서 태어났다.\"\n",
    "        parsed = frameparsing(sent)\n",
    "\n",
    "        graph = graphGeneration.conll2graph(parsed)\n",
    "        for triple in graph:\n",
    "            print(triple)\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장수: 27\n",
      "26 24 109 68\n"
     ]
    }
   ],
   "source": [
    "def test_example():\n",
    "    with open('./tmp/20180727.txt','r') as f:\n",
    "        d = f.readlines()\n",
    "    sentences = []\n",
    "    for i in d:\n",
    "        sentences.append(i.strip())\n",
    "    print('문장수:', len(sentences))\n",
    "    \n",
    "    result = open('./tmp/20170727_result.txt','w')\n",
    "    s_f_count, s_fe_count = 0,0\n",
    "    f_count, fe_count = 0,0\n",
    "\n",
    "    for sent in sentences:\n",
    "        result.write(sent+'\\n')\n",
    "        try:\n",
    "            parsed = frameparsing(sent)\n",
    "            graph = graphGeneration.conll2graph(parsed)\n",
    "            if len(graph) > 0:\n",
    "                s_f_count += 1\n",
    "                add = False\n",
    "                for i in range(len(graph)):\n",
    "                    t = graph[i]\n",
    "                    s,p,o = str(t[0]),str(t[1]),'\\\"'+str(t[2])+'\\\"'\n",
    "                    if i+1 == len(graph):\n",
    "                        end_mark = '.\\n'\n",
    "                    else:\n",
    "                        if t[0] == graph[i+1][0]:\n",
    "                            end_mark = ';\\n'\n",
    "                        else:\n",
    "                            end_mark = '.\\n'\n",
    "\n",
    "                    if 'frdf:lu' in t[1]:\n",
    "                        result.write(s + '\\t' + p + '\\t' + o + '\\t' + end_mark)\n",
    "                    else:\n",
    "                        result.write('\\t'+ p + '\\t' + o + '\\t' + end_mark)\n",
    "\n",
    "                    if 'frdf:lu' not in t[1]:\n",
    "                        fe_count += 1\n",
    "                        add = True\n",
    "                    else:\n",
    "                        f_count += 1\n",
    "                if add:\n",
    "                    s_fe_count += 1\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            pass\n",
    "        result.write('\\n')\n",
    "    \n",
    "    stat = '#sent: '+str(len(sentences))+', #sent_f: '+str(s_f_count)+', #sent_fe: '+str(s_fe_count)+', #frame: '+str(f_count)+', #fe: '+str(fe_count)\n",
    "    print(stat)\n",
    "    result.write(stat)\n",
    "    result.close()\n",
    "        \n",
    "test_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_test():\n",
    "    with open('/disk_4/cnndata/result_sample_corpus.json','r') as f:\n",
    "        cnn = json.load(f)\n",
    "    sentences = []\n",
    "    for i in cnn:\n",
    "        sentences.append(i['ko_sentence'])\n",
    "    print('CNN 문장수:', len(sentences))\n",
    "    s_f_count, s_fe_count = 0,0\n",
    "    f_count, fe_count = 0,0\n",
    "    \n",
    "    cnn_result = open('./tmp/cnnresult.rulebased.txt','w')\n",
    "\n",
    "    for sent in sentences:\n",
    "        cnn_result.write(sent+'\\n')\n",
    "        try:\n",
    "            parsed = frameparsing(sent)\n",
    "            graph = graphGeneration.conll2graph(parsed)\n",
    "            if len(graph) > 0:\n",
    "                s_f_count += 1\n",
    "                add = False\n",
    "                for t in graph:\n",
    "                    cnn_result.write(str(t)+'\\n')\n",
    "                    if 'frdf:lu' not in t[1]:\n",
    "                        fe_count += 1\n",
    "                        add = True\n",
    "                    else:\n",
    "                        f_count += 1\n",
    "                if add:\n",
    "                    s_fe_count += 1\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            pass\n",
    "        cnn_result.write('\\n')\n",
    "    \n",
    "    stat = '#sent: '+str(len(sentences))+', #sent_f: '+str(s_f_count)+', #sent_fe: '+str(s_fe_count)+', #frame: '+str(f_count)+', #fe: '+str(fe_count)\n",
    "    print(s_f_count, s_fe_count, f_count, fe_count)\n",
    "    cnn_result.write(stat)\n",
    "#cnn_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "4\n",
      "17\n",
      "{'sent_id': 1, 'text': '제임스 얼 \"지미\" 카터 주니어_(영화)(, 1924년 10월_1일 ~ )는 민주당_(미국) 출신 미국 39번째 대통령 (1977년 ~ 1981년)이다.', 'dbpedia': [('민주당_(미국)', 'headquarter', '미국'), ('민주당_(미국)', 'country', '미국')], 'frame': [('frame:Origin', 'frdf:lu', '출신.n'), ('frame:Origin', 'fe:entity', '미국 39번째'), ('frame:Leadership', 'frdf:lu', '대통령.n'), ('frame:Origin', 'frdf:lu', '미국.n'), ('frame:Leadership', 'fe:governed', '미국 39번째')]}\n",
      "\n",
      "{'sent_id': 4, 'text': '지미_카터는 조지아_주 섬터 카운티 플레인스 마을에서 태어났다.', 'dbpedia': [('지미_카터', 'region', '조지아_주'), ('지미_카터', 'residence', '조지아_주'), ('지미_카터', 'birthPlace', '조지아_주')], 'frame': [('frame:Political_locales', 'frdf:lu', '마을.n'), ('frame:Being_born', 'frdf:lu', '태어나다.v')]}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "def get_triples_from_ds(line):\n",
    "    sent = line[0]\n",
    "    sbj = re.search('\\<e1\\>(.*?)\\<\\/e1\\>', sent).group(1)\n",
    "    obj = re.search('\\<e2\\>(.*?)\\<\\/e2\\>', sent).group(1)\n",
    "    pred = line[1]\n",
    "    triple = (sbj, pred, obj)\n",
    "    return triple\n",
    "\n",
    "def ds_test():\n",
    "    ds_file = open('/disk_4/dsData/usingELU/ds_label_sen.csv','r', encoding='utf-8')\n",
    "    rdr = csv.reader(ds_file)\n",
    "    sent_ids = set()\n",
    "\n",
    "    result = []\n",
    "    n = 0\n",
    "    triples = []\n",
    "    for i in rdr:\n",
    "        try:\n",
    "            sent = i[0]\n",
    "            sent = sent.replace(\"<e1>\", \"\")\n",
    "            sent = sent.replace(\"</e1> \", \"\")\n",
    "            sent = sent.replace(\"</e1>\", \"\")\n",
    "            sent = sent.replace(\"<e2>\", \"\")\n",
    "            sent = sent.replace(\"</e2> \", \"\")\n",
    "            sent = sent.replace(\"</e2>\", \"\")\n",
    "            sent = sent.replace(\"[\", \"\")\n",
    "            sent = sent.replace(\"] \", \"\")\n",
    "            sent = sent.replace(\"]\", \"\")\n",
    "            sent_id = int(i[4])\n",
    "            if sent_id not in sent_ids:\n",
    "                n = n+1\n",
    "                print('sent_id: ', str(sent_id), '(processed :', str(n))\n",
    "                sent_ids.add(sent_id)\n",
    "                parsed = frameparsing(sent)\n",
    "                graph = graphGeneration.conll2graph(parsed)\n",
    "                triples = []\n",
    "                #graph = []\n",
    "            else:\n",
    "                pass\n",
    "            triple = get_triples_from_ds(i)\n",
    "            if sent_id not in sent_ids:\n",
    "                pass\n",
    "            else:\n",
    "                triples.append(triple)\n",
    "            d = {}\n",
    "            d['sent_id'] = int(sent_id)\n",
    "            d['text'] = sent\n",
    "            d['dbpedia'] = triples\n",
    "            d['frame'] = list(set(graph))\n",
    "\n",
    "            add = True\n",
    "            for r in result:\n",
    "                if sent_id == r['sent_id']:\n",
    "                    old_triples = d['dbpedia']\n",
    "                    new_triples = old_triples + triples\n",
    "                    new_triples = list(set(new_triples))\n",
    "                    r['dbpedia'] = new_triples\n",
    "                    add = False\n",
    "\n",
    "                else:\n",
    "                    pass\n",
    "            if add == True:\n",
    "                result.append(d)\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            pass\n",
    "#          n = n+1\n",
    "#         if n > 5:\n",
    "#             break\n",
    "        \n",
    "    ds_file.close()\n",
    "\n",
    "    with open('/disk_4/dsData/ds_result.json','w') as f:\n",
    "        json.dump(result, f, ensure_ascii=False, indent=4)\n",
    "    print(len(result))\n",
    "\n",
    "            \n",
    "ds_test()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### loading data now...\n",
      "# training_data\n",
      " - number of sentences: 3220\n",
      " - number of annotations: 12431 \n",
      "\n",
      "# test_data\n",
      " - number of sentences: 1124\n",
      " - number of annotations: 4382 \n",
      "\n",
      "# dev_data\n",
      " - number of sentences: 183\n",
      " - number of annotations: 624 \n",
      "\n",
      "# exemplar data (from sejong)\n",
      " - number of sentences: 10967\n",
      " - number of annotations: 10967 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import preprocessor\n",
    "# trn, tst, dev, exemplar = preprocessor.load_data()\n",
    "# preprocessor.data_stat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2642 1409 331\n",
      "1003 565 120\n"
     ]
    }
   ],
   "source": [
    "# n,v,a = [],[],[]\n",
    "# for i in tst:\n",
    "#     for j in i:\n",
    "#         if j[12] != '_':\n",
    "#             lu = j[12]\n",
    "#     pos = lu.split('.')[1]\n",
    "#     if pos == 'n':\n",
    "#         n.append(lu)\n",
    "#     elif pos == 'v':\n",
    "#         v.append(lu)\n",
    "#     else:\n",
    "#         a.append(lu)\n",
    "# print(len(n),len(v), len(a))\n",
    "# print(len(list(set(n))), len(list(set(v))), len(list(set(a))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
