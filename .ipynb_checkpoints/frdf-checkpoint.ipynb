{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from src import etri\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import time\n",
    "from koreanframenet import kfn\n",
    "from optparse import OptionParser\n",
    "import configparser\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import targetid\n",
    "import frameid\n",
    "import argid\n",
    "import graphGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PARSER SETTINGS\n",
      "_____________________\n",
      "Target Identification     \tbaseline\n",
      "Frame Identification      \tfrequent\n",
      "Argument Identification   \trulebased\n",
      "\n",
      "INPUT FILE IS           \t/disk_4/KFparser/test_example.tsv\n",
      "RESULT WILL BE SAVED TO  \t/disk_4/KFparser/test_output.tsv"
     ]
    }
   ],
   "source": [
    "# option\n",
    "targetid_model = 'baseline'\n",
    "frameid_model = 'frequent'\n",
    "argid_model = 'rulebased'\n",
    "\n",
    "# config\n",
    "config_file = '/home/iterative/summarization/summary.ini'\n",
    "# config_file = './test_summary.ini'\n",
    "config = configparser.ConfigParser()\n",
    "config.read(config_file)\n",
    "input_path = config.get('FRDF', 'FRDF_input_path')\n",
    "output_path = config.get('FRDF', 'FRDF_output_path')\n",
    "\n",
    "#print options\n",
    "sys.stderr.write(\"\\nPARSER SETTINGS\\n_____________________\\n\")\n",
    "sys.stderr.write(\"Target Identification     \\t\" + str(targetid_model) + '\\n')\n",
    "sys.stderr.write(\"Frame Identification      \\t\" + str(frameid_model) + '\\n')\n",
    "sys.stderr.write(\"Argument Identification   \\t\" + str(argid_model) + '\\n')\n",
    "sys.stderr.write(\"\\nINPUT FILE IS           \\t\" + str(input_path))\n",
    "sys.stderr.write(\"\\nRESULT WILL BE SAVED TO  \\t\" + str(output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parset options\n",
    "target_identifier = targetid.target_identifier\n",
    "frame_identifier = frameid.frame_identifier\n",
    "arg_identifier = argid.arg_identifier\n",
    "\n",
    "def frameparsing(sent):\n",
    "    conll = etri.getETRI_CoNLL2009(sent)\n",
    "    conll_target = target_identifier(conll, targetid_model)\n",
    "    conll_frame = frame_identifier(conll_target, frameid_model)\n",
    "    conll_arg = arg_identifier(conll_frame, argid_model)\n",
    "    \n",
    "    return conll_arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    data = []\n",
    "    f =  open(input_path)\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.rstrip().split('\\t')\n",
    "        d = {}\n",
    "        d['text'], d['docid'], d['pid'], d['sid'] = line[0], line[1], line[2], line[3]\n",
    "        data.append(d)\n",
    "    f.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBIO_list(sent_list):\n",
    "    result = []\n",
    "    fe_list = []\n",
    "    for i in range(len(sent_list)):\n",
    "        fe = sent_list[i][14].lower()\n",
    "        fe_list.append(fe)\n",
    "    for i in range(len(sent_list)):\n",
    "        fe = sent_list[i][14].lower()\n",
    "        if i == 0:\n",
    "            if fe == '_' or fe =='o':\n",
    "                fe = 'O'\n",
    "            else:\n",
    "                try:\n",
    "                    if fe == fe_list[i+1]:\n",
    "                        fe = 'B_'+fe\n",
    "                    else:\n",
    "                        fe = 'S_'+fe\n",
    "                except KeyboardInterrupt:\n",
    "                    raise\n",
    "                except:\n",
    "                    fe = 'S_'+fe\n",
    "        else:\n",
    "            if fe == '_' or fe =='o':\n",
    "                fe = 'O'\n",
    "            else:\n",
    "                if fe != fe_list[i-1]:\n",
    "                    try:\n",
    "                        if fe == fe_list[i+1]:\n",
    "                            fe = 'B_'+fe\n",
    "                        else:\n",
    "                            fe = 'S_'+fe\n",
    "                    except KeyboardInterrupt:\n",
    "                        raise\n",
    "                    except:\n",
    "                        fe = 'S_'+fe\n",
    "                else:\n",
    "                    fe = 'I_'+fe\n",
    "        result.append(fe)\n",
    "    for i in range(len(sent_list)):\n",
    "        sent_list[i][14] = result[i]\n",
    "    return sent_list\n",
    "\n",
    "def get_span(sent_list):\n",
    "    sent_list = getBIO_list(sent_list)\n",
    "    begin = False\n",
    "    result = []\n",
    "    for i in range(len(sent_list)):\n",
    "        token = sent_list[i]\n",
    "        fe = False\n",
    "        if token[14] != 'O':\n",
    "            fe = token[14].split('_')[1]\n",
    "        next_fe = False\n",
    "        if i+1 < len(sent_list):\n",
    "            if sent_list[i+1][14].startswith('I'):\n",
    "                next_fe = True\n",
    "        if token[14].startswith('S'):\n",
    "            d = {}\n",
    "            d['begin'], d['end'], d['tag'] = token[0], token[0], fe, \n",
    "            result.append(d)\n",
    "        else:\n",
    "            if token[14].startswith('B'):\n",
    "                begin = token[0]\n",
    "            elif token[14].startswith('I'):\n",
    "                end = token[0]\n",
    "                if next_fe:\n",
    "                    pass\n",
    "                else:\n",
    "                    d = {}\n",
    "                    d['begin'], d['end'], d['tag'] = begin, end, fe, \n",
    "                    result.append(d)\n",
    "    frame, begin, end = False, False, False\n",
    "    for token in sent_list:\n",
    "        if token[13] != '_':\n",
    "            frame = token[13]\n",
    "            if begin == False:\n",
    "                begin = token[0]\n",
    "                end = token[0]\n",
    "            else:\n",
    "                end = token[0]\n",
    "    if frame:\n",
    "        d = {}\n",
    "        d['begin'], d['end'], d['tag']= begin, end, 'frame:'+frame\n",
    "        result.append(d)\n",
    "    return result   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_annotation(parsed):\n",
    "    result = []\n",
    "    for sent_list in parsed:\n",
    "        anno = get_span(sent_list)\n",
    "        tokens = []\n",
    "        for token in sent_list:\n",
    "            tokens.append(token[1])\n",
    "        for a in anno:\n",
    "            begin,end, tag = a['begin'], a['end'], a['tag']\n",
    "            \n",
    "            for i in range(len(tokens)):\n",
    "                if i == begin:\n",
    "                    tokens[i] = '['+tokens[i]\n",
    "                if i == end:\n",
    "                    tokens[i] = tokens[i]+']/'+tag\n",
    "        annotation = ' '.join(tokens)\n",
    "        result.append(annotation)\n",
    "    return result\n",
    "input_data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "RESULT\n",
      "_____________________\n",
      "PROCESSED SENTENCES     \t2 sentences\n",
      "PROCESSING TIME         \t0.29\n",
      "ANNOTATIONS             \t4 annotations\n"
     ]
    }
   ],
   "source": [
    "def parsing():\n",
    "    result = []\n",
    "    for i in input_data:\n",
    "        parsed = frameparsing(i['text'])\n",
    "        annos = gen_annotation(parsed)\n",
    "        for anno in annos:\n",
    "            annotation = anno + '\\t' +  i['docid'] + '\\t' + i['pid'] + '\\t' + i['sid']\n",
    "            result.append(annotation)\n",
    "    return result\n",
    "\n",
    "def write_result(result):\n",
    "    time_spent = time.time()-start_time\n",
    "    sys.stderr.write(\"\\nRESULT\\n_____________________\\n\")\n",
    "    sys.stderr.write(\"PROCESSED SENTENCES     \\t\" + str(len(input_data)) + ' sentences\\n')\n",
    "    sys.stderr.write(\"PROCESSING TIME         \\t\" + str(round(time_spent, 2)) + '\\n')\n",
    "    sys.stderr.write(\"ANNOTATIONS             \\t\" + str(len(result)) + ' annotations\\n')\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for i in result:\n",
    "            f.write(i+'\\n')\n",
    "\n",
    "start_time = time.time()\n",
    "result = parsing()\n",
    "write_result(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
